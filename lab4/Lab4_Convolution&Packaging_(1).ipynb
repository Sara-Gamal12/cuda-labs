{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SACSa2bT_vc7"
      },
      "source": [
        "### **Cuda Programming Applications**\n",
        "\n",
        "This mini-lab targets some hands-on implementations and more practice on cuda in common real-world recurring tasks. Moreover, we aim to compare the outcomes of our low-level implementations with the built-in functions in popular frameworks as Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DU7eXap6ZpW"
      },
      "source": [
        "### **Requirement**\n",
        "\n",
        "A) A cuda program is required to carry out a 3D convolution over RGB images and save the output ones, the program is given a path to a folder containing the input images and that of an output folder that should contain the outputs, respectively as command line arguments.\n",
        "\n",
        "1.   kernel1: basic implementation (no tiling)\n",
        "2.   kernel2: tiling where each block matches the input tile size.\n",
        "\n",
        "Notes:\n",
        "*   Add necessary paddings so that the output image size is the same as that of the input one.\n",
        "\n",
        "*   The kernel should be able to handle a batch of images at a time, the batch size is passed as the 3rd argument.\n",
        "*   The mask is given in a .txt file, whose path is passed as the 4th argument. The first line contains its dimension n (one number only as it's a square mask) then the consecutive n lines contain the mask rows, each row in a separate line.\n",
        "\n",
        "  Ex: ./a.out input_folder_path output_folder_path 4 mask.txt\n",
        "\n",
        "B) Implement the same program in python, using the built-in convolution functions in Pytorch.\n",
        "\n",
        "C) Profile each program carefully and do sufficient experiments to compare between them and collect insightful results. Organise your results in a tabular form and prepare a comprehensive report with visual graphs explaining all of your findings. Also mention the impact of declaring the mask as constant in terms of execution time and elaborate on this in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc-0dlsfvh1n"
      },
      "source": [
        "#### **Helpers**\n",
        "\n",
        "This section contains some helpers that could be needed for the requirement. Check it frequently.\n",
        "\n",
        "**Helper1**: Read RGB images in C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piCQVytHWUH1",
        "outputId": "0612c80a-7af0-4ae9-d568-32307040d816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-0vip98iw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-0vip98iw\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10742 sha256=fdb30b442a03a2b978d94f242f4f3839c3b517111d4c5bcc31496f40b5aacbba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2ua8qwry/wheels/ef/1d/c6/f7e47f1aa1bc9d05c4120d94f90a79cf28603ef343b0dd43ff\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp8u61tzi_\".\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCT_z26pv281",
        "outputId": "2ecfcf76-66ce-4390-8ffd-bc7c2c43e214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stb'...\n",
            "remote: Enumerating objects: 8138, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8138 (delta 1), reused 0 (delta 0), pack-reused 8132 (from 1)\u001b[K\n",
            "Receiving objects: 100% (8138/8138), 5.64 MiB | 9.98 MiB/s, done.\n",
            "Resolving deltas: 100% (5400/5400), done.\n"
          ]
        }
      ],
      "source": [
        "# Fetch stb_image library\n",
        "!git clone https://github.com/nothings/stb.git\n",
        "!cp stb/stb_image.h /usr/local/include/\n",
        "!cp stb/stb_image_write.h /usr/local/include/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0n4jdBLdQ_1"
      },
      "source": [
        "# **kernel1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxxREmm5gA4X",
        "outputId": "d616cde3-b141-46c2-d999-d4e82b4b54f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting kernel1.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile kernel1.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <assert.h>\n",
        "#include <string.h>\n",
        "#include <dirent.h>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <float.h>\n",
        "#include<iostream>\n",
        "#define STB_IMAGE_IMPLEMENTATION\n",
        "#include \"stb_image.h\"\n",
        "#define STB_IMAGE_WRITE_IMPLEMENTATION\n",
        "#include \"stb_image_write.h\"\n",
        "\n",
        "#include <sys/stat.h>\n",
        "__constant__ float c_mask[256];\n",
        "\n",
        "\n",
        "void save_images(const char* output_folder, float* output_data,   int width, int height, int channels,  int batch_size,std::vector<std::string> input_paths,int batch_start) {\n",
        "    // Create output directory if it doesn't exist\n",
        "    mkdir(output_folder, 0777);\n",
        "\n",
        "\n",
        "    // Process each image in the current batch\n",
        "    for (int i = 0; i < batch_size; i++) {\n",
        "\n",
        "        // Extract filename from input path\n",
        "        std::string path = input_paths[ i+batch_start];\n",
        "\n",
        "        size_t last_slash = path.find_last_of(\"/\\\\\");\n",
        "        std::string filename = (last_slash == std::string::npos) ? path : path.substr(last_slash + 1);\n",
        "\n",
        "        // Create output path (preserve extension)\n",
        "        std::string output_path = std::string(output_folder) + \"/conv_\" + filename;\n",
        "\n",
        "        // Allocate memory for output image (convert from float to uint8)\n",
        "        unsigned char* image_data = (unsigned char*)malloc(width * height*channels );\n",
        "\n",
        "      float min_pixel = FLT_MAX;\n",
        "      float max_pixel = -FLT_MAX;\n",
        "\n",
        "  for (int k=0;k<channels;k++)\n",
        "       for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "\n",
        "          int output_idx = ((i*channels+k )* height * width ) +        (y * width ) +   (x ) ;\n",
        "            if (output_data[output_idx] < min_pixel)\n",
        "                min_pixel = output_data[output_idx];\n",
        "            if (output_data[output_idx] > max_pixel)\n",
        "                max_pixel = output_data[output_idx];\n",
        "        }}\n",
        "        // Convert and normalize output data\n",
        "        for(int k=0;k<channels;k++)\n",
        "        for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "              {\n",
        "                    // Calculate indices (NHWC layout)\n",
        "                    int output_idx = ((i*channels+k ) * height * width ) +\n",
        "                                   (y * width ) +\n",
        "                                   (x ) ;\n",
        "\n",
        "                    float pixel_val = output_data[output_idx];\n",
        "\n",
        "                    pixel_val=static_cast<unsigned char>(255.0f *(pixel_val-min_pixel)/(max_pixel-min_pixel));\n",
        "                   image_data[((y * width + x)*channels+k) ] = pixel_val;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Save image (preserve original format)\n",
        "        std::string ext = filename.substr(filename.find_last_of(\".\") + 1);\n",
        "        int success = 0;\n",
        "        if (ext == \"png\") {\n",
        "            success = stbi_write_png(output_path.c_str(), width, height, channels, image_data, width * channels);\n",
        "        }\n",
        "        else if (ext == \"jpg\" || ext == \"jpeg\") {\n",
        "            success = stbi_write_jpg(output_path.c_str(), width, height, channels, image_data, 90);  // 90% quality\n",
        "        }\n",
        "        else {\n",
        "            printf(\"Unsupported output format for %s, defaulting to PNG\\n\", output_path.c_str());\n",
        "            success = stbi_write_png(output_path.c_str(), width, height, 1, image_data, width * 1);\n",
        "        }\n",
        "\n",
        "        if (!success) {\n",
        "            printf(\"Failed to save image %s\\n\", output_path.c_str());\n",
        "        }\n",
        "\n",
        "        free(image_data);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv3D_basic(const uint8_t *input, int width, int height, int depth,int batch_size, float *output, float *mask, int maskWidth)\n",
        " {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int batch_index= threadIdx.z+blockIdx.z * blockDim.z;\n",
        "\n",
        "\n",
        "    if (col >= width || row >= height||batch_index>=batch_size ) return;\n",
        "\n",
        "\n",
        "   for (int channel=0;channel<depth;channel++)\n",
        "{\n",
        "   float sum = 0.0f;\n",
        "    for (int i = 0; i < maskWidth; ++i) {\n",
        "        for (int j =0; j < maskWidth; ++j) {\n",
        "\n",
        "            int curr_row = row+i-maskWidth/2;\n",
        "            int curr_col = col+j-maskWidth/2;\n",
        "            if(curr_col<width&& curr_row<height&&curr_col>=0&&curr_row>=0)\n",
        "            {\n",
        "\n",
        "             {\n",
        "               sum+=mask[i*maskWidth+j]*static_cast<float>(input[batch_index*height*width*depth  +  curr_row*width*depth+curr_col*depth+channel]);\n",
        "\n",
        "}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "      int outIdx = (batch_index*depth+channel)*height*width+row*width+col;\n",
        "               output[outIdx] = sum;}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "float* read_mask(const char* file_path, int& maskWidth) {\n",
        "    FILE* file = fopen(file_path, \"r\");\n",
        "    if (!file) {\n",
        "        fprintf(stderr, \"Error: Could not open mask file %s\\n\", file_path);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    // Read mask dimension (first line)\n",
        "    if (fscanf(file, \"%d\", &maskWidth) != 1) {\n",
        "        fprintf(stderr, \"Error: Could not read mask dimension from %s\\n\", file_path);\n",
        "        fclose(file);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    float* mask = (float*)malloc(maskWidth * maskWidth * sizeof(float));\n",
        "    if (!mask) {\n",
        "        fprintf(stderr, \"Error: Memory allocation failed for mask\\n\");\n",
        "        fclose(file);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    // Read mask values (subsequent lines)\n",
        "    for (int i = 0; i < maskWidth; i++) {\n",
        "        for (int j = 0; j < maskWidth; j++) {\n",
        "            if (fscanf(file, \"%f\", &mask[i * maskWidth + j]) != 1) {\n",
        "                fprintf(stderr, \"Error: Invalid mask data at row %d, column %d\\n\", i+1, j+1);\n",
        "                free(mask);\n",
        "                fclose(file);\n",
        "                return nullptr;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fclose(file);\n",
        "    return mask;\n",
        "}\n",
        "\n",
        "\n",
        "uint8_t* load_images(const char* folder_path, int& width, int& height, int& channels, int batch_size,int & num_images, std::vector<std::string>& image_paths) {\n",
        "    DIR *dir;\n",
        "    struct dirent *ent;\n",
        "\n",
        "    if ((dir = opendir(folder_path)) != NULL) {\n",
        "        while ((ent = readdir(dir)) != NULL) {\n",
        "            std::string filename = ent->d_name;\n",
        "            if (filename.find(\".jpg\") != std::string::npos ||\n",
        "                filename.find(\".jpeg\") != std::string::npos ||\n",
        "                filename.find(\".png\") != std::string::npos) {\n",
        "                image_paths.push_back(std::string(folder_path) + \"/\" + filename);\n",
        "            }\n",
        "        }\n",
        "        closedir(dir);\n",
        "    } else {\n",
        "        perror(\"Could not open directory\");\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    if (image_paths.empty()) {\n",
        "        printf(\"No images found in %s\\n\", folder_path);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "   num_images=image_paths.size();\n",
        "   uint8_t* h_input;\n",
        "    // Load images into batch\n",
        "    for (int i = 0; i < image_paths.size(); i++) {\n",
        "        int img_width, img_height, img_channels;\n",
        "        unsigned char* image_data = stbi_load(image_paths[i].c_str(), &img_width, &img_height, &img_channels, 0);\n",
        "\n",
        "        if(i==0)\n",
        "        {\n",
        "           height=img_height;\n",
        "        width=img_width;\n",
        "        channels=img_channels;\n",
        "           size_t input_size = image_paths.size() * height * width * channels * sizeof(uint8_t);\n",
        "            h_input = (uint8_t*)malloc(input_size);\n",
        "\n",
        "        }\n",
        "        if (!image_data) {\n",
        "            printf(\"Failed to load image: %s\\n\", image_paths[i].c_str());\n",
        "            continue;\n",
        "        }\n",
        "\n",
        "\n",
        "        // Copy image data to batch (NHWC layout)\n",
        "        for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "                for (int c = 0; c < channels; c++) {\n",
        "                    int src_idx = (y * width + x) * channels + c;\n",
        "                    int dst_idx = (i * height * width * channels) +\n",
        "                                 (y * width * channels) +\n",
        "                                 (x * channels) + c;\n",
        "                    h_input[dst_idx] = image_data[src_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        stbi_image_free(image_data);\n",
        "    }\n",
        "\n",
        "    return h_input;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "\n",
        "   if (argc != 5) {\n",
        "        printf(\"arguments are incorrect\");\n",
        "        return 1;\n",
        "    }\n",
        "    const char* input_folder = argv[1];\n",
        "    const char* output_folder = argv[2];\n",
        "    int batch_size = atoi(argv[3]);\n",
        "    const char* mask_file = argv[4];\n",
        "\n",
        "\n",
        "\n",
        "    int maskWidth;\n",
        "    float*h_mask=read_mask(mask_file,maskWidth);\n",
        "    if(!h_mask)\n",
        "    {\n",
        "      return 1;\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    int height,width,depth;\n",
        "    uint8_t* h_input;\n",
        "    int num_images;\n",
        "    std::vector<std::string> input_paths;\n",
        "    h_input=load_images(input_folder,width,height,depth,batch_size, num_images,input_paths);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    uint8_t* d_input;\n",
        "    float* d_output;\n",
        "    float* d_mask;\n",
        "\n",
        "    //Allocate\n",
        "\n",
        "\n",
        "    cudaMalloc(&d_mask, maskWidth * maskWidth * sizeof(float));\n",
        "    cudaMemcpy(d_mask, h_mask, maskWidth * maskWidth * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(c_mask, h_mask, maskWidth * maskWidth * sizeof(float));\n",
        "\n",
        "    for(int batch_start=0;batch_start<num_images;batch_start+=batch_size)\n",
        "    {\n",
        "\n",
        "        int current_batch_size = (batch_start + batch_size > num_images) ? num_images - batch_start : batch_size;\n",
        "        size_t input_size = current_batch_size * height * width * sizeof(uint8_t)*depth;\n",
        "        size_t output_size = current_batch_size * height * width * sizeof(float)*depth;;\n",
        "         float* h_output = (float*)malloc(output_size);\n",
        "\n",
        "            //copy to gpu\n",
        "            cudaMalloc(&d_input, input_size);\n",
        "            cudaMalloc(&d_output, output_size);\n",
        "\n",
        "\n",
        "            cudaMemcpy(d_input,  &h_input[batch_start * width * height * depth], input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "   dim3 block_size(16, 16, 1);\n",
        "   dim3 grid_size(\n",
        "       (width + block_size.x - 1) / block_size.x,\n",
        "       (height + block_size.y - 1) / block_size.y,\n",
        "       current_batch_size\n",
        "\n",
        "   );\n",
        "\n",
        "\n",
        "            conv3D_basic<<<grid_size, block_size>>>(d_input, width, height, depth, current_batch_size,\n",
        "                d_output, d_mask, maskWidth);\n",
        "\n",
        "            cudaError_t err = cudaGetLastError();\n",
        "            if (err != cudaSuccess) {\n",
        "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
        "            }\n",
        "\n",
        "            cudaDeviceSynchronize();  // Required to flush printf output\n",
        "\n",
        "\n",
        "            cudaMemcpy(h_output, d_output, output_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "            save_images(output_folder,h_output,width,height,depth,current_batch_size,input_paths,batch_start);\n",
        "            cudaFree(d_input);\n",
        "            cudaFree(d_output);\n",
        "            free(h_output);\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "  free(h_mask);\n",
        "    free(h_input);\n",
        "    cudaFree(d_mask);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb3YzYjPI21i",
        "outputId": "def5b848-558a-4478-a4f5-231698ace197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==26149== NVPROF is profiling process 26149, command: ./kernel1.out /content/input /content/kernel11 4 /content/mask.txt\n",
            "==26149== Profiling application: ./kernel1.out /content/input /content/kernel11 4 /content/mask.txt\n",
            "==26149== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   90.46%  83.627ms         2  41.814ms  26.617ms  57.010ms  [CUDA memcpy DtoH]\n",
            "                    6.52%  6.0255ms         4  1.5064ms     640ns  4.0796ms  [CUDA memcpy HtoD]\n",
            "                    3.02%  2.7942ms         2  1.3971ms  934.10us  1.8601ms  conv3D_basic(unsigned char const *, int, int, int, int, float*, float*, int)\n",
            "      API calls:   64.67%  178.97ms         5  35.794ms  101.36us  178.42ms  cudaMalloc\n",
            "                   33.35%  92.286ms         5  18.457ms  21.388us  58.009ms  cudaMemcpy\n",
            "                    1.02%  2.8274ms         2  1.4137ms  965.86us  1.8615ms  cudaDeviceSynchronize\n",
            "                    0.77%  2.1443ms         5  428.87us  157.07us  884.70us  cudaFree\n",
            "                    0.08%  209.52us         1  209.52us  209.52us  209.52us  cudaMemcpyToSymbol\n",
            "                    0.06%  160.92us       114  1.4110us     116ns  68.390us  cuDeviceGetAttribute\n",
            "                    0.03%  91.586us         2  45.793us  41.363us  50.223us  cudaLaunchKernel\n",
            "                    0.01%  14.180us         1  14.180us  14.180us  14.180us  cuDeviceGetName\n",
            "                    0.00%  12.559us         2  6.2790us     410ns  12.149us  cudaGetLastError\n",
            "                    0.00%  5.1150us         1  5.1150us  5.1150us  5.1150us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4830us         3     494ns     167ns     923ns  cuDeviceGetCount\n",
            "                    0.00%  1.0280us         2     514ns     291ns     737ns  cuDeviceGet\n",
            "                    0.00%     592ns         1     592ns     592ns     592ns  cuDeviceTotalMem\n",
            "                    0.00%     385ns         1     385ns     385ns     385ns  cuModuleGetLoadingMode\n",
            "                    0.00%     252ns         1     252ns     252ns     252ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 kernel1.cu -o kernel1.out\n",
        "!nvprof ./kernel1.out /content/input /content/kernel11 4 /content/mask.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAgKpnNzf729"
      },
      "source": [
        "# **kernel2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1bHJOgmvWN",
        "outputId": "964dd9f9-659c-4797-a6b6-f64b5e54329d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting kernel2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile kernel2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <assert.h>\n",
        "#include <string.h>\n",
        "#include <dirent.h>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <float.h>\n",
        "#include<iostream>\n",
        "#define STB_IMAGE_IMPLEMENTATION\n",
        "#include \"stb_image.h\"\n",
        "#define STB_IMAGE_WRITE_IMPLEMENTATION\n",
        "#include \"stb_image_write.h\"\n",
        "\n",
        "#include <sys/stat.h>\n",
        "# define OUTPUT_TILE_DIM 16\n",
        "__constant__ float c_mask[256];\n",
        "void save_images(const char* output_folder, float* output_data,   int width, int height, int channels,  int batch_size,std::vector<std::string> input_paths,int batch_start) {\n",
        "    // Create output directory if it doesn't exist\n",
        "    mkdir(output_folder, 0777);\n",
        "\n",
        "\n",
        "    // Process each image in the current batch\n",
        "    for (int i = 0; i < batch_size; i++) {\n",
        "\n",
        "        // Extract filename from input path\n",
        "        std::string path = input_paths[ i+batch_start];\n",
        "\n",
        "        size_t last_slash = path.find_last_of(\"/\\\\\");\n",
        "        std::string filename = (last_slash == std::string::npos) ? path : path.substr(last_slash + 1);\n",
        "\n",
        "        // Create output path (preserve extension)\n",
        "        std::string output_path = std::string(output_folder) + \"/conv_\" + filename;\n",
        "\n",
        "        // Allocate memory for output image (convert from float to uint8)\n",
        "        unsigned char* image_data = (unsigned char*)malloc(width * height*channels );\n",
        "\n",
        "      float min_pixel = FLT_MAX;\n",
        "       float max_pixel = -FLT_MAX;\n",
        "\n",
        "  for (int k=0;k<channels;k++)\n",
        "       for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "\n",
        "          int output_idx = ((i*channels+k )* height * width ) +        (y * width ) +   (x ) ;\n",
        "            if (output_data[output_idx] < min_pixel)\n",
        "                min_pixel = output_data[output_idx];\n",
        "            if (output_data[output_idx] > max_pixel)\n",
        "                max_pixel = output_data[output_idx];\n",
        "        }}\n",
        "        // Convert and normalize output data\n",
        "        for(int k=0;k<channels;k++)\n",
        "        for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "              {\n",
        "                    // Calculate indices (NHWC layout)\n",
        "                    int output_idx = ((i*channels+k ) * height * width ) +\n",
        "                                   (y * width ) +\n",
        "                                   (x ) ;\n",
        "\n",
        "                    float pixel_val = output_data[output_idx];\n",
        "\n",
        "                    pixel_val=static_cast<unsigned char>(255.0f *(pixel_val-min_pixel)/(max_pixel-min_pixel));\n",
        "                   image_data[((y * width + x)*channels+k) ] = pixel_val;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Save image (preserve original format)\n",
        "        std::string ext = filename.substr(filename.find_last_of(\".\") + 1);\n",
        "        int success = 0;\n",
        "        if (ext == \"png\") {\n",
        "            success = stbi_write_png(output_path.c_str(), width, height, channels, image_data, width * channels);\n",
        "        }\n",
        "        else if (ext == \"jpg\" || ext == \"jpeg\") {\n",
        "            success = stbi_write_jpg(output_path.c_str(), width, height, channels, image_data, 90);  // 90% quality\n",
        "        }\n",
        "        else {\n",
        "            printf(\"Unsupported output format for %s, defaulting to PNG\\n\", output_path.c_str());\n",
        "            success = stbi_write_png(output_path.c_str(), width, height, 1, image_data, width * 1);\n",
        "        }\n",
        "\n",
        "        if (!success) {\n",
        "            printf(\"Failed to save image %s\\n\", output_path.c_str());\n",
        "        }\n",
        "\n",
        "        free(image_data);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv3D_tiled(const uint8_t *input, int width, int height, int depth,int batch_size, float *output, float *mask, int maskWidth)\n",
        " {\n",
        "    int  input_tile_dim=OUTPUT_TILE_DIM+maskWidth-1;\n",
        "     extern  __shared__ float tile [];\n",
        "\n",
        "     int tx=threadIdx.x;\n",
        "     int ty=threadIdx.y;\n",
        "\n",
        "    int col = blockIdx.x * OUTPUT_TILE_DIM  + threadIdx.x;\n",
        "    int row = blockIdx.y *OUTPUT_TILE_DIM + threadIdx.y;\n",
        "    int batch_index= threadIdx.z+blockIdx.z*blockDim.z;\n",
        "\n",
        "\n",
        "    int shared_col=col-maskWidth/2;\n",
        "    int shared_row=row-maskWidth/2;\n",
        "\n",
        "\n",
        "    for(int channel=0;channel<depth;channel++)\n",
        "   {\n",
        "    if(shared_col<0||shared_col>=width||shared_row>=height||shared_row<0||batch_index>=batch_size)\n",
        "       tile[ty*input_tile_dim+tx]=0;\n",
        "       else\n",
        "      tile[ty*input_tile_dim+tx]=static_cast<float>(input[batch_index*width*depth*height+shared_row*width*depth+shared_col*depth+channel]);\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        " float sum = 0.0f;\n",
        "if (tx < OUTPUT_TILE_DIM && ty < OUTPUT_TILE_DIM && col < width && row < height) {\n",
        "\n",
        "    for (int i = 0; i < maskWidth; ++i) {\n",
        "        for (int j =0; j < maskWidth; ++j) {\n",
        "\n",
        "            int curr_row = i+ty;\n",
        "            int curr_col =j+tx;\n",
        "            if(curr_col<width&& curr_row<height&&curr_col>=0&&curr_row>=0)\n",
        "            {\n",
        "\n",
        "              sum+=mask[i*maskWidth+j]*static_cast<float>(tile[curr_row*input_tile_dim+curr_col]);\n",
        "\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    int outIdx = (batch_index*depth+channel)*height*width+row*width+col;\n",
        "    output[outIdx] = sum;}\n",
        "      __syncthreads();\n",
        "}\n",
        "}\n",
        "\n",
        "\n",
        "float* read_mask(const char* file_path, int& maskWidth) {\n",
        "    FILE* file = fopen(file_path, \"r\");\n",
        "    if (!file) {\n",
        "        fprintf(stderr, \"Error: Could not open mask file %s\\n\", file_path);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    // Read mask dimension (first line)\n",
        "    if (fscanf(file, \"%d\", &maskWidth) != 1) {\n",
        "        fprintf(stderr, \"Error: Could not read mask dimension from %s\\n\", file_path);\n",
        "        fclose(file);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    float* mask = (float*)malloc(maskWidth * maskWidth * sizeof(float));\n",
        "    if (!mask) {\n",
        "        fprintf(stderr, \"Error: Memory allocation failed for mask\\n\");\n",
        "        fclose(file);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    // Read mask values (subsequent lines)\n",
        "    for (int i = 0; i < maskWidth; i++) {\n",
        "        for (int j = 0; j < maskWidth; j++) {\n",
        "            if (fscanf(file, \"%f\", &mask[i * maskWidth + j]) != 1) {\n",
        "                fprintf(stderr, \"Error: Invalid mask data at row %d, column %d\\n\", i+1, j+1);\n",
        "                free(mask);\n",
        "                fclose(file);\n",
        "                return nullptr;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fclose(file);\n",
        "    return mask;\n",
        "}\n",
        "\n",
        "\n",
        "uint8_t* load_images(const char* folder_path, int& width, int& height, int& channels, int batch_size,int & num_images, std::vector<std::string>& image_paths) {\n",
        "    DIR *dir;\n",
        "    struct dirent *ent;\n",
        "\n",
        "    if ((dir = opendir(folder_path)) != NULL) {\n",
        "        while ((ent = readdir(dir)) != NULL) {\n",
        "            std::string filename = ent->d_name;\n",
        "            if (filename.find(\".jpg\") != std::string::npos ||\n",
        "                filename.find(\".jpeg\") != std::string::npos ||\n",
        "                filename.find(\".png\") != std::string::npos) {\n",
        "                image_paths.push_back(std::string(folder_path) + \"/\" + filename);\n",
        "            }\n",
        "        }\n",
        "        closedir(dir);\n",
        "    } else {\n",
        "        perror(\"Could not open directory\");\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "    if (image_paths.empty()) {\n",
        "        printf(\"No images found in %s\\n\", folder_path);\n",
        "        return nullptr;\n",
        "    }\n",
        "\n",
        "   num_images=image_paths.size();\n",
        "   uint8_t* h_input;\n",
        "    // Load images into batch\n",
        "    for (int i = 0; i < image_paths.size(); i++) {\n",
        "        int img_width, img_height, img_channels;\n",
        "        unsigned char* image_data = stbi_load(image_paths[i].c_str(), &img_width, &img_height, &img_channels, 0);\n",
        "\n",
        "        if(i==0)\n",
        "        {\n",
        "           height=img_height;\n",
        "        width=img_width;\n",
        "        channels=img_channels;\n",
        "           size_t input_size = image_paths.size() * height * width * channels * sizeof(uint8_t);\n",
        "            h_input = (uint8_t*)malloc(input_size);\n",
        "\n",
        "        }\n",
        "        if (!image_data) {\n",
        "            printf(\"Failed to load image: %s\\n\", image_paths[i].c_str());\n",
        "            continue;\n",
        "        }\n",
        "\n",
        "\n",
        "        // Copy image data to batch (NHWC layout)\n",
        "        for (int y = 0; y < height; y++) {\n",
        "            for (int x = 0; x < width; x++) {\n",
        "                for (int c = 0; c < channels; c++) {\n",
        "                    int src_idx = (y * width + x) * channels + c;\n",
        "                    int dst_idx = (i * height * width * channels) +\n",
        "                                 (y * width * channels) +\n",
        "                                 (x * channels) + c;\n",
        "                    h_input[dst_idx] = image_data[src_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        stbi_image_free(image_data);\n",
        "    }\n",
        "\n",
        "    return h_input;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "\n",
        "   if (argc != 5) {\n",
        "        printf(\"arguments are incorrect\");\n",
        "        return 1;\n",
        "    }\n",
        "    const char* input_folder = argv[1];\n",
        "    const char* output_folder = argv[2];\n",
        "    int batch_size = atoi(argv[3]);\n",
        "    const char* mask_file = argv[4];\n",
        "\n",
        "\n",
        "\n",
        "    int maskWidth;\n",
        "    float*h_mask=read_mask(mask_file,maskWidth);\n",
        "    if(!h_mask)\n",
        "    {\n",
        "      return 1;\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    int height,width,depth;\n",
        "    uint8_t* h_input;\n",
        "    int num_images;\n",
        "    std::vector<std::string> input_paths;\n",
        "    h_input=load_images(input_folder,width,height,depth,batch_size, num_images,input_paths);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    uint8_t* d_input;\n",
        "    float* d_output;\n",
        "    float* d_mask;\n",
        "\n",
        "    //Allocate\n",
        "\n",
        "\n",
        "    cudaMalloc(&d_mask, maskWidth * maskWidth * sizeof(float));\n",
        "    cudaMemcpy(d_mask, h_mask, maskWidth * maskWidth * sizeof(float), cudaMemcpyHostToDevice);\n",
        "      cudaMemcpyToSymbol(c_mask, h_mask, maskWidth * maskWidth * sizeof(float));\n",
        "\n",
        "    for(int batch_start=0;batch_start<num_images;batch_start+=batch_size)\n",
        "    {\n",
        "\n",
        "        int current_batch_size = (batch_start + batch_size > num_images) ? num_images - batch_start : batch_size;\n",
        "        size_t input_size = current_batch_size * height * width * sizeof(uint8_t)*depth;\n",
        "        size_t output_size = current_batch_size * height * width * sizeof(float)*depth;;\n",
        "         float* h_output = (float*)malloc(output_size);\n",
        "\n",
        "            //copy to gpu\n",
        "            cudaMalloc(&d_input, input_size);\n",
        "            cudaMalloc(&d_output, output_size);\n",
        "\n",
        "\n",
        "            cudaMemcpy(d_input,  &h_input[batch_start * width * height * depth], input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "   dim3 block_size(OUTPUT_TILE_DIM+maskWidth-1, OUTPUT_TILE_DIM+maskWidth-1, 1);\n",
        "   dim3 grid_size(\n",
        "       (width + OUTPUT_TILE_DIM - 1) / OUTPUT_TILE_DIM,\n",
        "       (height +OUTPUT_TILE_DIM - 1) / OUTPUT_TILE_DIM,\n",
        "       current_batch_size\n",
        "\n",
        "   );\n",
        "\n",
        "              int sharedMemorySize = sizeof(float) * (OUTPUT_TILE_DIM + maskWidth - 1) * (OUTPUT_TILE_DIM + maskWidth - 1);\n",
        "\n",
        "            conv3D_tiled<<<grid_size, block_size,sharedMemorySize>>>(d_input, width, height, depth, current_batch_size,\n",
        "                d_output, d_mask, maskWidth);\n",
        "\n",
        "            cudaError_t err = cudaGetLastError();\n",
        "            if (err != cudaSuccess) {\n",
        "            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
        "            }\n",
        "\n",
        "            cudaDeviceSynchronize();  // Required to flush printf output\n",
        "\n",
        "\n",
        "            cudaMemcpy(h_output, d_output, output_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "            save_images(output_folder,h_output,width,height,depth,current_batch_size,input_paths,batch_start);\n",
        "            cudaFree(d_input);\n",
        "            cudaFree(d_output);\n",
        "            free(h_output);\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "  free(h_mask);\n",
        "    free(h_input);\n",
        "    cudaFree(d_mask);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smz_u1TTthhQ",
        "outputId": "61f355a3-5f78-4bc1-da26-87fe71aa698d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==26577== NVPROF is profiling process 26577, command: ./kernel2.out /content/input /content/kernel2 4 /content/mask.txt\n",
            "==26577== Profiling application: ./kernel2.out /content/input /content/kernel2 4 /content/mask.txt\n",
            "==26577== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   89.01%  83.659ms         2  41.829ms  26.565ms  57.093ms  [CUDA memcpy DtoH]\n",
            "                    6.51%  6.1222ms         4  1.5306ms     673ns  4.0872ms  [CUDA memcpy HtoD]\n",
            "                    4.48%  4.2065ms         2  2.1032ms  1.4044ms  2.8021ms  conv3D_tiled(unsigned char const *, int, int, int, int, float*, float*, int)\n",
            "      API calls:   64.57%  181.08ms         5  36.215ms  113.78us  180.44ms  cudaMalloc\n",
            "                   32.99%  92.505ms         5  18.501ms  23.175us  58.107ms  cudaMemcpy\n",
            "                    1.52%  4.2582ms         2  2.1291ms  1.4518ms  2.8064ms  cudaDeviceSynchronize\n",
            "                    0.74%  2.0805ms         5  416.10us  144.22us  871.79us  cudaFree\n",
            "                    0.08%  221.05us         1  221.05us  221.05us  221.05us  cudaMemcpyToSymbol\n",
            "                    0.06%  167.79us       114  1.4710us     120ns  59.223us  cuDeviceGetAttribute\n",
            "                    0.04%  99.601us         2  49.800us  33.224us  66.377us  cudaLaunchKernel\n",
            "                    0.00%  12.915us         1  12.915us  12.915us  12.915us  cuDeviceGetName\n",
            "                    0.00%  4.3670us         1  4.3670us  4.3670us  4.3670us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.1240us         3     374ns     141ns     722ns  cuDeviceGetCount\n",
            "                    0.00%  1.0740us         2     537ns     138ns     936ns  cuDeviceGet\n",
            "                    0.00%     718ns         2     359ns     320ns     398ns  cudaGetLastError\n",
            "                    0.00%     575ns         1     575ns     575ns     575ns  cuModuleGetLoadingMode\n",
            "                    0.00%     412ns         1     412ns     412ns     412ns  cuDeviceTotalMem\n",
            "                    0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvcc -arch=sm_75 kernel2.cu -o kernel2.out\n",
        "!nvprof ./kernel2.out /content/input /content/kernel2 4 /content/mask.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dfIQM6qlcUo",
        "outputId": "08d6bfd1-a6ec-452b-bc61-10df59d2ed73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVzUSHUkleAQ",
        "outputId": "150fedc3-4d5e-4164-9b08-c936a227b153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting kernel.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile kernel.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def load_mask(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "        n = int(lines[0])\n",
        "        mask_vals = [list(map(float, line.strip().split())) for line in lines[1:]]\n",
        "        mask = torch.tensor(mask_vals, dtype=torch.float32)\n",
        "    return mask.view(1, 1, 1, n, n), n  # Shape: [1, 1, 1, k, k]\n",
        "\n",
        "def load_input_images(input_folder):\n",
        "    input_files = sorted(os.listdir(input_folder))\n",
        "    input_images = []\n",
        "    for input_file in input_files:\n",
        "        input_path = os.path.join(input_folder, input_file)\n",
        "        input_images.append(Image.open(input_path).convert(\"RGB\"))\n",
        "    return input_images\n",
        "\n",
        "def pytorch_convolution(input_images, kernel, kernel_size, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using\", device)\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "    outputs = []\n",
        "    timings = {'total_execution_time_ms': 0, 'batch_times_ms': []}\n",
        "\n",
        "    kernel = kernel.to(device)\n",
        "\n",
        "    for i in range(0, len(input_images), batch_size):\n",
        "        batch_images = input_images[i:i+batch_size]\n",
        "        batch_tensors = [transform(img).unsqueeze(0) for img in batch_images]\n",
        "        batch_tensor = torch.cat(batch_tensors, dim=0).to(device)  # [B, 3, H, W]\n",
        "\n",
        "        B, C, H, W = batch_tensor.shape\n",
        "\n",
        "        # Reshape for 3D: Treat RGB as 'depth'\n",
        "        batch_tensor = batch_tensor.view(B, C, 1, H, W)  # [B, C, D=1, H, W]\n",
        "        batch_tensor = batch_tensor.permute(0, 2, 1, 3, 4)  # [B, 1, C, H, W]\n",
        "\n",
        "        # Pad (H, W) only\n",
        "        padding = kernel_size // 2\n",
        "        batch_tensor_padded = F.pad(batch_tensor, (padding, padding, padding, padding), mode='constant', value=0)\n",
        "\n",
        "        # Convolve\n",
        "        batch_start_time = time.time()\n",
        "        output = F.conv3d(batch_tensor_padded, kernel)\n",
        "        batch_end_time = time.time()\n",
        "\n",
        "        # Reshape back\n",
        "        output = output.permute(0, 2, 3, 4, 1).squeeze(-1)  # [B, C, H, W]\n",
        "        output = output.cpu().numpy()\n",
        "\n",
        "        for img_array in output:\n",
        "            outputs.append(img_array)\n",
        "\n",
        "        timings['batch_times_ms'].append((batch_end_time - batch_start_time) * 1000)\n",
        "\n",
        "    timings['total_execution_time_ms'] = sum(timings['batch_times_ms'])\n",
        "    return outputs, timings\n",
        "\n",
        "def save_output(output_images, output_path):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    for i, img in enumerate(output_images):\n",
        "        img = (img - img.min()) / (img.max() - img.min() + 1e-5)  # normalize to [0,1]\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        img = np.transpose(img, (1, 2, 0))  # [H, W, C]\n",
        "        Image.fromarray(img).save(os.path.join(output_path, f\"pytorch_output_{i}.jpg\"))\n",
        "\n",
        "def profile(timings):\n",
        "    print(\"Total execution time of F.conv3d (ms):\", timings['total_execution_time_ms'])\n",
        "    for i, batch_time in enumerate(timings['batch_times_ms']):\n",
        "        print(f\"\\tBatch {i+1} execution time (ms): {batch_time}\")\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 5:\n",
        "        print(\"Usage: python B_1_17_2_14.py <input_folder> <output_folder> <batch_size> <mask_file>\")\n",
        "        return\n",
        "\n",
        "    input_folder = sys.argv[1]\n",
        "    output_folder = sys.argv[2]\n",
        "    batch_size = int(sys.argv[3])\n",
        "    mask_file = sys.argv[4]\n",
        "\n",
        "    kernel, kernel_size = load_mask(mask_file)\n",
        "    input_images = load_input_images(input_folder)\n",
        "\n",
        "    output_images, timings = pytorch_convolution(input_images, kernel, kernel_size, batch_size)\n",
        "    save_output(output_images, output_folder)\n",
        "    profile(timings)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8E9DMLgm0tR",
        "outputId": "9ab7e792-3ac6-4838-cfba-ab1797ca4cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n",
            "Total execution time of F.conv3d (ms): 88.82594108581543\n",
            "\tBatch 1 execution time (ms): 86.7612361907959\n",
            "\tBatch 2 execution time (ms): 2.0647048950195312\n"
          ]
        }
      ],
      "source": [
        "!python kernel.py /content/input /content/python 4 /content/mask.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG4A7cxUl_Ja"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
